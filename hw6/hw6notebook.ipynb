{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "stuff=np.load(\"data.npz\")\n",
    "X_trn = stuff[\"X_trn\"]\n",
    "y_trn = stuff[\"y_trn\"]\n",
    "X_tst = stuff[\"X_tst\"]\n",
    "# no Y_tst !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Helper Function\n",
    "\n",
    "# import numpy as np\n",
    "# import csv\n",
    "\n",
    "# def write_csv(y_pred, filename):\n",
    "#     \"\"\"Write a 1d numpy array to a Kaggle-compatible .csv file\"\"\"\n",
    "#     with open(filename, 'w') as csv_file:\n",
    "#         csv_writer = csv.writer(csv_file)\n",
    "#         csv_writer.writerow(['Id', 'Category'])\n",
    "#         for idx, y in enumerate(y_pred):\n",
    "#             csv_writer.writerow([idx, y])\n",
    "\n",
    "\n",
    "# data = np.load('data.npz')\n",
    "# X_tst = data['X_tst']\n",
    "# y_pred = np.random.randint(0, 3, size=len(X_tst)) # random predictions\n",
    "# write_csv(y_pred, 'sample_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example plot\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# def show(x):\n",
    "#     img = x.reshape((3,29,29)).transpose(1,2,0)\n",
    "#     plt.imshow(img)\n",
    "#     plt.axis('off')\n",
    "#     plt.draw()\n",
    "#     plt.pause(0.01)\n",
    "\n",
    "# show(X_trn[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split | Information Gain\n",
      " 0.5  |  0.0\n",
      " 1.5  |  0.17095059445466854\n",
      " 2.5  |  0.4199730940219748\n",
      " 3.5  |  0.01997309402197489\n",
      " 4.5  |  0.17095059445466854\n",
      " 5.5  |  0.0\n"
     ]
    }
   ],
   "source": [
    "X = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "Y = [1, 1, 0, 0, 1]\n",
    "splits = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "\n",
    "# calculate parent entropy\n",
    "\n",
    "parent_p1 = sum(Y) / len(Y)\n",
    "parent_p0 = 1 - parent_p1\n",
    "parent_entropy = (-parent_p1*np.log2(parent_p1)) + (- parent_p0*np.log2(parent_p0))\n",
    "\n",
    "# calculate child entropy for each split\n",
    "child_entropy = {.5: -1, 1.5: -1, 2.5: -1, 3.5: -1, 4.5: -1, 5.5: -1}\n",
    "\n",
    "for split in splits:\n",
    "    # split data\n",
    "    left = []\n",
    "    right = []\n",
    "    for i in range(len(X)):\n",
    "        if X[i] < split:\n",
    "            left.append(Y[i])\n",
    "        else:\n",
    "            right.append(Y[i])\n",
    "\n",
    "    # calculate I(p1) and I(p2) for both left and right sides (if empty, set to 0)\n",
    "    left_p1 = 0\n",
    "    if len(left) != 0:\n",
    "        left_p1 = sum(left) / len(left)\n",
    "    left_p0 = 1 - left_p1\n",
    "\n",
    "    right_p1 = 0\n",
    "    if len(right) != 0:\n",
    "        right_p1 = sum(right) / len(right)\n",
    "    right_p0 = 1 - right_p1\n",
    "\n",
    "    if left_p1 == 0:\n",
    "        left_entropy = -left_p0*np.log2(left_p0)\n",
    "    elif left_p0 == 0:\n",
    "        left_entropy = -left_p1*np.log2(left_p1)\n",
    "    else:\n",
    "        left_entropy = (-left_p1*np.log2(left_p1)) + (- left_p0*np.log2(left_p0))\n",
    "    \n",
    "    if right_p1 == 0:\n",
    "        right_entropy = -right_p0*np.log2(right_p0)\n",
    "    elif right_p0 == 0:\n",
    "        right_entropy = -right_p1*np.log2(right_p1)\n",
    "    else:\n",
    "        right_entropy = (-right_p1*np.log2(right_p1)) + (- right_p0*np.log2(right_p0))\n",
    "    \n",
    "    # calculate child entropy and store\n",
    "    child_entropy[split] = ((len(left))*left_entropy + (len(right))*right_entropy) / len(Y)\n",
    "\n",
    "# calculate information gain and print\n",
    "information_gain = {split: parent_entropy - child_entropy[split] for split in splits}\n",
    "\n",
    "print(\"Split | Information Gain\")\n",
    "for split in splits:\n",
    "    print(f\" {split}  |  {information_gain[split]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassificationStump():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def fit(self, X_trn, y_trn):\n",
    "        \n",
    "        # do stuff here\n",
    "        D = len(X_trn[0])\n",
    "        N = len(X_trn)\n",
    "        dim = -1\n",
    "        thresh = -1\n",
    "        min_error = np.inf\n",
    "        c_left = 0\n",
    "        c_right = 0\n",
    "\n",
    "        for i in range(D):\n",
    "\n",
    "            sorted_indices = np.argsort(X_trn[:, i])\n",
    "            Z = X_trn[sorted_indices]\n",
    "            y_sorted = y_trn[sorted_indices]\n",
    "\n",
    "\n",
    "            for n in range(N-1):\n",
    "                t = (Z[n][i] + Z[n+1][i])/2\n",
    "\n",
    "                R1 = y_sorted[:n+1] # x <= t\n",
    "                R2 = y_sorted[n+1:] # x > t\n",
    "\n",
    "                R1_count = np.bincount(R1, minlength=np.max(y_sorted) + 1)\n",
    "                R2_count = np.bincount(R2, minlength=np.max(y_sorted) + 1)\n",
    "                \n",
    "                c1 = np.argmax(R1_count)\n",
    "                c2 = np.argmax(R2_count)\n",
    "\n",
    "                p1 = R1_count / len(R1) # probability of each class in R1\n",
    "                p2 = R2_count / len(R2) # probability of each class in R2\n",
    "\n",
    "                gini_left = np.sum(p1 * (1-p1)) # sum pi(1-pi) for each class in R1\n",
    "                gini_right = np.sum(p2 * (1-p2))\n",
    "                \n",
    "                gini_total = (len(R1) / N) * gini_left + (len(R2) / N) * gini_right # weighted average of gini impurity\n",
    "                \n",
    "                error = gini_total\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    dim = i\n",
    "                    c_left = c1\n",
    "                    c_right = c2\n",
    "                    thresh = t\n",
    "\n",
    "        self.model = (dim, thresh, c_left, c_right)\n",
    "        self.model = {'dim': dim, 'thresh': thresh, 'c_left': c_left, 'c_right': c_right}\n",
    "        return \n",
    "\n",
    "    def predict(self, X_val, y_val=None):\n",
    "        assert hasattr(self, \"model\"), \"No fitted model!\"\n",
    "        # do stuff here (use self.model for prediction)\n",
    "        y_pred = []\n",
    "        for x_val in X_val:\n",
    "            y_pred.append(self.model['c_left'] if x_val[self.model['dim']] <= self.model['thresh'] else self.model['c_right'])\n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error:  0.6421666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "clf = ClassificationStump()\n",
    "\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3 * 29 * 29))\n",
    "\n",
    "\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_trn)\n",
    "correct_pred = sum(y_pred == y_trn)\n",
    "e = 1 - correct_pred / len(y_pred)\n",
    "print(\"Training classification error: \", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DEPTH | TRAINING ERROR\n",
      "  1  | 0.6433333333333333\n",
      "  3  | 0.5515\n",
      "  6  | 0.4033333333333333\n",
      "  9  | 0.2493333333333333\n",
      "  12  | 0.1293333333333333\n",
      "  14  | 0.07150000000000001\n"
     ]
    }
   ],
   "source": [
    "# train classifcation trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), -1))\n",
    "\n",
    "max_depths = [1,3,6,9,12,14]\n",
    "# return a 6x1 table of classification errors\n",
    "print(\" DEPTH | TRAINING ERROR\")\n",
    "for max_depth in max_depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    y_pred = clf.predict(X_trn)\n",
    "    correct_pred = sum(y_pred == y_trn)\n",
    "    acc = correct_pred / len(y_trn)\n",
    "    e = 1 - acc \n",
    "    print(f\"  {max_depth}  | {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LAMBDA | TRAINING ERROR | LOGISTIC LOSS\n",
      "  0.1  | 0.17000000000000004 | 0.4771340095214954\n",
      "  1  | 0.21383333333333332 | 0.582162089440951\n",
      "  10  | 0.26449999999999996 | 0.6939481457195188\n",
      "  100  | 0.3031666666666667 | 0.7863802988369172\n",
      "  1000  | 0.33666666666666667 | 0.877910964337898\n"
     ]
    }
   ],
   "source": [
    "# linear classifier model with logistic loss and ridge regularization only using sklearn.linear and decision_function() method\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3*29*29))  \n",
    "\n",
    "\n",
    "lambda_vals = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# report training classification error and logistic loss for each lambda value\n",
    "print(\" LAMBDA | TRAINING ERROR | LOGISTIC LOSS\")\n",
    "for lambda_val in lambda_vals:\n",
    "    clf = LogisticRegression(penalty='l2', C=1/lambda_val, max_iter=10000, tol=.001)\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    \n",
    "    confidence_scores = clf.decision_function(X_trn) # get confidence score per class per sample \n",
    "   \n",
    "    exp_scores = np.exp(confidence_scores) # prepare for softmax\n",
    "    sum_exp_scores_per_sample = [sum(scores) for scores in exp_scores]\n",
    "\n",
    "    probs = [exp_scores[i] / sum_exp_scores_per_sample[i] for i in range(len(exp_scores))] # softmax to get probs\n",
    "    y_pred = [np.argmax(prob) for prob in probs] # get prediction from highest prob\n",
    "    logistic_loss = log_loss(y_trn, probs) # calculate logistic loss\n",
    "   \n",
    "    correct_preds = np.sum(y_pred == y_trn)\n",
    "    e = 1 - (correct_preds / len(y_trn)) # calculate classification error\n",
    "    print(f\"  {lambda_val}  | {e} | {logistic_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K | TRAINING ERROR\n",
      " 1 | 0.0\n",
      " 3 | 0.29200000000000004\n",
      " 5 | 0.31633333333333336\n",
      " 7 | 0.3478333333333333\n",
      " 9 | 0.3626666666666667\n",
      " 11 | 0.3663333333333333\n"
     ]
    }
   ],
   "source": [
    "# K nearest neighbors classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), -1))\n",
    "\n",
    "k_neighbors = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "print(\" K | TRAINING ERROR\")\n",
    "for k in k_neighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    y_pred = clf.predict(X_trn)\n",
    "    correct_pred = sum(y_pred == y_trn)\n",
    "    acc = correct_pred / len(y_trn)\n",
    "    e = 1 - acc\n",
    "    print(f\" {k} | {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        return\n",
    "\n",
    "    def fit(self, X_trn, y_trn):\n",
    "        self.model.fit(X_trn, y_trn)\n",
    "        # self.model is stored\n",
    "        return\n",
    "\n",
    "    def predict(self, X_val, y_val=None):\n",
    "        # self.model is used\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def cross_validation(classifier, X_trn, y_trn, n_folds=5):\n",
    "    # do stuff here\n",
    "\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    outputs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_trn):\n",
    "        \n",
    "        x_train, x_test = X_trn[train_index], X_trn[test_index]\n",
    "        y_train, y_test = y_trn[train_index], y_trn[test_index]\n",
    "        \n",
    "        \n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred = classifier.predict(x_test)\n",
    "        correct_preds = np.sum(y_pred == y_test)\n",
    "        e = 1 - (correct_preds / len(y_test))\n",
    "        \n",
    "        outputs.append((classifier, e))\n",
    "    \n",
    "    # Return the paired (model and error) for all folds\n",
    "    return outputs # [(model1, error1), (model2, error2), ..., (modelK, errorK)]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAX_DEPTH | FOLD_1_ERROR | FOLD_2_ERROR | FOLD_3_ERROR | FOLD_4_ERROR | FOLD_5_ERROR | AVG_ERROR\n",
      " 1 | 0.6541666666666667 | 0.6616666666666666 | 0.6358333333333333 | 0.6375 | 0.63 | 0.6438333333333333\n",
      " 3 | 0.5275000000000001 | 0.5691666666666666 | 0.565 | 0.5475 | 0.54 | 0.5498333333333333\n",
      " 6 | 0.4866666666666667 | 0.4933333333333333 | 0.4966666666666667 | 0.48250000000000004 | 0.4783333333333334 | 0.4875\n",
      " 9 | 0.4675 | 0.4966666666666667 | 0.4783333333333334 | 0.4691666666666666 | 0.45833333333333337 | 0.47400000000000003\n",
      " 12 | 0.46499999999999997 | 0.4966666666666667 | 0.475 | 0.4675 | 0.4675 | 0.47433333333333333\n",
      " 14 | 0.47916666666666663 | 0.5058333333333334 | 0.475 | 0.4633333333333334 | 0.4766666666666667 | 0.48\n"
     ]
    }
   ],
   "source": [
    "# Usage for the cross_validation function:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3 * 29 * 29))\n",
    "\n",
    "print(\" MAX_DEPTH | FOLD_1_ERROR | FOLD_2_ERROR | FOLD_3_ERROR | FOLD_4_ERROR | FOLD_5_ERROR | AVG_ERROR\")\n",
    "for max_depth in [1, 3, 6, 9, 12, 14]:\n",
    "    classifier = Classifier(DecisionTreeClassifier(max_depth=max_depth))\n",
    "    N = 5\n",
    "    outputs = cross_validation(classifier, \n",
    "                           X_trn, \n",
    "                           y_trn, \n",
    "                           n_folds=N)\n",
    "    print(f\" {max_depth} | \", end=\"\")\n",
    "    for out_ in outputs:\n",
    "        print(f\"{out_[1]} | \", end=\"\")\n",
    "    avg_error = np.mean([out_[1] for out_ in outputs])\n",
    "    print(f\"{avg_error}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LAMBDA | FOLD_1_ERROR | FOLD_2_ERROR | FOLD_3_ERROR | FOLD_4_ERROR | FOLD_5_ERROR | AVG_ERROR\n",
      " 0.1 | 0.4408333333333333 | 0.4491666666666667 | 0.46166666666666667 | 0.46166666666666667 | 0.45166666666666666 | 0.45299999999999996\n",
      " 1 | 0.4158333333333334 | 0.4391666666666667 | 0.43500000000000005 | 0.43333333333333335 | 0.4066666666666666 | 0.426\n",
      " 10 | 0.38583333333333336 | 0.405 | 0.39416666666666667 | 0.4008333333333334 | 0.3533333333333334 | 0.38783333333333336\n",
      " 100 | 0.3433333333333334 | 0.38 | 0.345 | 0.3616666666666667 | 0.3308333333333333 | 0.3521666666666667\n",
      " 1000 | 0.3458333333333333 | 0.37 | 0.3533333333333334 | 0.35583333333333333 | 0.3325 | 0.35150000000000003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3 * 29 * 29))\n",
    "# y_trn = y_trn.reshape((len(y_trn), -1)) # OH: CHECK THIS, changes the answer -> needed to work\n",
    "\n",
    "\n",
    "print(\" LAMBDA | FOLD_1_ERROR | FOLD_2_ERROR | FOLD_3_ERROR | FOLD_4_ERROR | FOLD_5_ERROR | AVG_ERROR\")\n",
    "for lambda_val in [0.1, 1, 10, 100, 1000]:\n",
    "    classifier = Classifier(LogisticRegression(penalty='l2', C=1/lambda_val, max_iter=10000, tol=.001))\n",
    "    N = 5\n",
    "    outputs = cross_validation(classifier, \n",
    "                           X_trn, \n",
    "                           y_trn, \n",
    "                           n_folds=N)\n",
    "    print(f\" {lambda_val} | \", end=\"\")\n",
    "    for out_ in outputs:\n",
    "        print(f\"{out_[1]} | \", end=\"\")\n",
    "    avg_error = np.mean([out_[1] for out_ in outputs]) \n",
    "    print(f\"{avg_error}\")\n",
    "\n",
    "# OH: SHOULDNT ERRORS INCREASE WITH LAMBDA? CHECK THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier():\n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        return\n",
    "\n",
    "    def fit(self, X_trn, y_trn):\n",
    "        # Just store X_trn, y_trn\n",
    "        self.model = (X_trn, y_trn)\n",
    "\n",
    "    def predict(self, X_val, y_val=None):\n",
    "        assert hasattr(self, \"model\"), \"No fitted model!\"\n",
    "        # do stuff here (use self.model for prediction)\n",
    "        \n",
    "        def KNN_predict_(X_trn, y_trn, x, K):\n",
    "            # Dictionary to store n: distance pairs\n",
    "            distances = {}\n",
    "            for n in range(len(X_trn)):\n",
    "                # Calculate distance between x and x[n]\n",
    "                distance = np.sqrt(np.sum((x - X_trn[n])**2))\n",
    "                distances[n] = distance\n",
    "            \n",
    "            # Sort distances in ascending order\n",
    "            sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "\n",
    "            # Get the K nearest neighbors\n",
    "            y_neighbor = []\n",
    "            for n in range(K):\n",
    "                n_nearest = sorted_distances[n][0]\n",
    "                y_neighbor.append(y_trn[n_nearest])\n",
    "            \n",
    "            # get majority class\n",
    "            c_pred = np.argmax(np.bincount(y_neighbor))\n",
    "            return c_pred\n",
    "        \n",
    "        X_trn, y_trn = self.model\n",
    "        y_pred = []\n",
    "        for x in X_val:\n",
    "            y_pred.append(KNN_predict_(X_trn, y_trn, x, self.n_neighbors))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K | FOLD_1_ERROR | FOLD_2_ERROR | FOLD_3_ERROR | FOLD_4_ERROR | FOLD_5_ERROR | AVG_ERROR\n",
      " 1 | 0.47 | 0.45833333333333337 | 0.4575 | 0.4408333333333333 | 0.4575 | 0.4568333333333333\n",
      " 3 | 0.48250000000000004 | 0.45833333333333337 | 0.4658333333333333 | 0.4541666666666667 | 0.4866666666666667 | 0.46950000000000003\n",
      " 5 | 0.45499999999999996 | 0.44166666666666665 | 0.45166666666666666 | 0.4325 | 0.46499999999999997 | 0.4491666666666666\n",
      " 7 | 0.43833333333333335 | 0.4291666666666667 | 0.44666666666666666 | 0.42666666666666664 | 0.44833333333333336 | 0.4378333333333333\n",
      " 9 | 0.43333333333333335 | 0.4258333333333333 | 0.43333333333333335 | 0.43333333333333335 | 0.44499999999999995 | 0.4341666666666667\n",
      " 11 | 0.4308333333333333 | 0.43500000000000005 | 0.4408333333333333 | 0.4275 | 0.4308333333333333 | 0.433\n"
     ]
    }
   ],
   "source": [
    "k_neighbors = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "print(\" K | FOLD_1_ERROR | FOLD_2_ERROR | FOLD_3_ERROR | FOLD_4_ERROR | FOLD_5_ERROR | AVG_ERROR\")\n",
    "for k in k_neighbors:\n",
    "    knn = KNNClassifier(k)\n",
    "    knn.fit(X_trn, y_trn)\n",
    "    models = cross_validation(knn, X_trn, y_trn, n_folds=5)\n",
    "    print(f\" {k} | \", end=\"\")\n",
    "    for model in models:\n",
    "        print(f\"{model[1]} | \", end=\"\")\n",
    "    avg_error = np.mean([model[1] for model in models])\n",
    "    print(f\"{avg_error}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model | hyper-parameters | cross-validation avg. error | public leaderboard accuracy\n",
    "\n",
    "Classification Tree | max_depth = 9 | .474 | x\n",
    "\n",
    "Logistic Regression | lambda = 10 | .388 | x\n",
    "\n",
    "KNN Classification | K = 7 | 0.438 | x\n",
    "\n",
    "Reasoning:\n",
    " - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def write_csv(y_pred, filename):\n",
    "    \"\"\"Write a 1d numpy array to a Kaggle-compatible .csv file\"\"\"\n",
    "    with open(filename, 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Id', 'Category'])\n",
    "        for idx, y in enumerate(y_pred):\n",
    "            csv_writer.writerow([idx, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best tree model on test data and save predictions\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "X_tst = X_tst.reshape((len(X_tst), 3 * 29 * 29))\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 9)\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "write_csv(y_pred, 'tree_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best logistic regression model on test data and save predictions\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "X_tst = X_tst.reshape((len(X_tst), 3 * 29 * 29))\n",
    "\n",
    "lambda_val = 10\n",
    "clf = LogisticRegression(penalty='l2', C=1/lambda_val, max_iter=10000, tol=.001)\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "write_csv(y_pred, 'logistic_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best k nearest neighbor on test data and save predictions\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "X_tst = X_tst.reshape((len(X_tst), 3 * 29 * 29))\n",
    "\n",
    "k = 7\n",
    "clf = KNNClassifier(k)\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "write_csv(y_pred, 'knn_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
