{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS589 ASSIGNMENT 6\\\n",
    "\\\n",
    "Name: Dorian Benhamou Goldfajn\\\n",
    "Email: dbenhamougol@umass.edu\\\n",
    "Discussed With: Aryan Nair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "stuff=np.load(\"data.npz\")\n",
    "X_trn = stuff[\"X_trn\"]\n",
    "y_trn = stuff[\"y_trn\"]\n",
    "X_tst = stuff[\"X_tst\"]\n",
    "# no Y_tst !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split | Information Gain\n",
      " 0.5  |  0.0\n",
      " 1.5  |  0.17095059445466854\n",
      " 2.5  |  0.4199730940219748\n",
      " 3.5  |  0.01997309402197489\n",
      " 4.5  |  0.17095059445466854\n",
      " 5.5  |  0.0\n"
     ]
    }
   ],
   "source": [
    "X = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "Y = [1, 1, 0, 0, 1]\n",
    "splits = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "\n",
    "# calculate parent entropy\n",
    "\n",
    "parent_p1 = sum(Y) / len(Y)\n",
    "parent_p0 = 1 - parent_p1\n",
    "parent_entropy = (-parent_p1*np.log2(parent_p1)) + (- parent_p0*np.log2(parent_p0))\n",
    "\n",
    "# calculate child entropy for each split\n",
    "child_entropy = {.5: -1, 1.5: -1, 2.5: -1, 3.5: -1, 4.5: -1, 5.5: -1}\n",
    "\n",
    "for split in splits:\n",
    "    # split data\n",
    "    left = []\n",
    "    right = []\n",
    "    for i in range(len(X)):\n",
    "        if X[i] < split:\n",
    "            left.append(Y[i])\n",
    "        else:\n",
    "            right.append(Y[i])\n",
    "\n",
    "    # calculate I(p1) and I(p2) for both left and right sides (if empty, set to 0)\n",
    "    left_p1 = 0\n",
    "    if len(left) != 0:\n",
    "        left_p1 = sum(left) / len(left)\n",
    "    left_p0 = 1 - left_p1\n",
    "\n",
    "    right_p1 = 0\n",
    "    if len(right) != 0:\n",
    "        right_p1 = sum(right) / len(right)\n",
    "    right_p0 = 1 - right_p1\n",
    "\n",
    "    if left_p1 == 0:\n",
    "        left_entropy = -left_p0*np.log2(left_p0)\n",
    "    elif left_p0 == 0:\n",
    "        left_entropy = -left_p1*np.log2(left_p1)\n",
    "    else:\n",
    "        left_entropy = (-left_p1*np.log2(left_p1)) + (- left_p0*np.log2(left_p0))\n",
    "    \n",
    "    if right_p1 == 0:\n",
    "        right_entropy = -right_p0*np.log2(right_p0)\n",
    "    elif right_p0 == 0:\n",
    "        right_entropy = -right_p1*np.log2(right_p1)\n",
    "    else:\n",
    "        right_entropy = (-right_p1*np.log2(right_p1)) + (- right_p0*np.log2(right_p0))\n",
    "    \n",
    "    # calculate child entropy and store\n",
    "    child_entropy[split] = ((len(left))*left_entropy + (len(right))*right_entropy) / len(Y)\n",
    "\n",
    "# calculate information gain and print\n",
    "information_gain = {split: parent_entropy - child_entropy[split] for split in splits}\n",
    "\n",
    "print(\"Split | Information Gain\")\n",
    "for split in splits:\n",
    "    print(f\" {split}  |  {information_gain[split]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassificationStump():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def fit(self, X_trn, y_trn):\n",
    "        \n",
    "        # do stuff here\n",
    "        D = len(X_trn[0])\n",
    "        N = len(X_trn)\n",
    "        dim = -1\n",
    "        thresh = -1\n",
    "        min_error = np.inf\n",
    "        c_left = 0\n",
    "        c_right = 0\n",
    "\n",
    "        for i in range(D):\n",
    "\n",
    "            sorted_indices = np.argsort(X_trn[:, i])\n",
    "            Z = X_trn[sorted_indices]\n",
    "            y_sorted = y_trn[sorted_indices]\n",
    "\n",
    "\n",
    "            for n in range(N-1):\n",
    "                t = (Z[n][i] + Z[n+1][i])/2\n",
    "\n",
    "                R1 = y_sorted[:n+1] # x <= t\n",
    "                R2 = y_sorted[n+1:] # x > t\n",
    "\n",
    "                R1_count = np.bincount(R1, minlength=np.max(y_sorted) + 1)\n",
    "                R2_count = np.bincount(R2, minlength=np.max(y_sorted) + 1)\n",
    "                \n",
    "                c1 = np.argmax(R1_count)\n",
    "                c2 = np.argmax(R2_count)\n",
    "\n",
    "                p1 = R1_count / len(R1) # probability of each class in R1\n",
    "                p2 = R2_count / len(R2) # probability of each class in R2\n",
    "\n",
    "                gini_left = np.sum(p1 * (1-p1)) # sum pi(1-pi) for each class in R1\n",
    "                gini_right = np.sum(p2 * (1-p2))\n",
    "                \n",
    "                gini_total = (len(R1) / N) * gini_left + (len(R2) / N) * gini_right # weighted average of gini impurity\n",
    "                \n",
    "                error = gini_total\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    dim = i\n",
    "                    c_left = c1\n",
    "                    c_right = c2\n",
    "                    thresh = t\n",
    "\n",
    "        self.model = (dim, thresh, c_left, c_right)\n",
    "        self.model = {'dim': dim, 'thresh': thresh, 'c_left': c_left, 'c_right': c_right}\n",
    "        return \n",
    "\n",
    "    def predict(self, X_val, y_val=None):\n",
    "        assert hasattr(self, \"model\"), \"No fitted model!\"\n",
    "        # do stuff here (use self.model for prediction)\n",
    "        y_pred = []\n",
    "        for x_val in X_val:\n",
    "            y_pred.append(self.model['c_left'] if x_val[self.model['dim']] <= self.model['thresh'] else self.model['c_right'])\n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error:  0.6421666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "clf = ClassificationStump()\n",
    "\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3 * 29 * 29))\n",
    "\n",
    "\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_trn)\n",
    "correct_pred = sum(y_pred == y_trn)\n",
    "e = 1 - correct_pred / len(y_pred)\n",
    "print(\"Training classification error: \", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DEPTH | TRAINING ERROR\n",
      "  1  | 0.6433333333333333\n",
      "  3  | 0.5515\n",
      "  6  | 0.4033333333333333\n",
      "  9  | 0.2493333333333333\n",
      "  12  | 0.1293333333333333\n",
      "  14  | 0.07150000000000001\n"
     ]
    }
   ],
   "source": [
    "# train classifcation trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "\n",
    "max_depths = [1,3,6,9,12,14]\n",
    "# return a 6x1 table of classification errors\n",
    "print(\" DEPTH | TRAINING ERROR\")\n",
    "for max_depth in max_depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    y_pred = clf.predict(X_trn)\n",
    "    correct_pred = sum(y_pred == y_trn)\n",
    "    acc = correct_pred / len(y_trn)\n",
    "    e = 1 - acc \n",
    "    print(f\"  {max_depth}  | {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of O(M). For each depth in the tree, we check a single threshold value (O(1)) until we get to the end of the tree where we make final classication, giving rise O(M * 1) = O(M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O(D * Nlogn). We must iterate through each D, and for each D we sort N items and iterate through N items. This is equivalent to O(D * (NlogN + N)) = O(D * NlogN), although I think it slightly depends on implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LAMBDA | TRAINING ERROR | LOGISTIC LOSS\n",
      "  0.1  | 0.17000000000000004 | 0.4771340095214954\n",
      "  1  | 0.21383333333333332 | 0.582162089440951\n",
      "  10  | 0.26449999999999996 | 0.6939481457195188\n",
      "  100  | 0.3031666666666667 | 0.7863802988369172\n",
      "  1000  | 0.33666666666666667 | 0.877910964337898\n"
     ]
    }
   ],
   "source": [
    "# linear classifier model with logistic loss and ridge regularization only using sklearn.linear and decision_function() method\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3*29*29))  \n",
    "\n",
    "\n",
    "lambda_vals = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# report training classification error and logistic loss for each lambda value\n",
    "print(\" LAMBDA | TRAINING ERROR | LOGISTIC LOSS\")\n",
    "for lambda_val in lambda_vals:\n",
    "    clf = LogisticRegression(penalty='l2', C=1/lambda_val, max_iter=10000, tol=.001)\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    \n",
    "    confidence_scores = clf.decision_function(X_trn) # get confidence score per class per sample \n",
    "   \n",
    "    exp_scores = np.exp(confidence_scores) # prepare for softmax\n",
    "    sum_exp_scores_per_sample = [sum(scores) for scores in exp_scores]\n",
    "\n",
    "    probs = [exp_scores[i] / sum_exp_scores_per_sample[i] for i in range(len(exp_scores))] # softmax to get probs\n",
    "    y_pred = [np.argmax(prob) for prob in probs] # get prediction from highest prob\n",
    "    logistic_loss = log_loss(y_trn, probs) # calculate logistic loss\n",
    "   \n",
    "    correct_preds = np.sum(y_pred == y_trn)\n",
    "    e = 1 - (correct_preds / len(y_trn)) # calculate classification error\n",
    "    print(f\"  {lambda_val}  | {e} | {logistic_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K | TRAINING ERROR\n",
      " 1 | 0.0\n",
      " 3 | 0.29200000000000004\n",
      " 5 | 0.31633333333333336\n",
      " 7 | 0.3478333333333333\n",
      " 9 | 0.3626666666666667\n",
      " 11 | 0.3663333333333333\n"
     ]
    }
   ],
   "source": [
    "# K nearest neighbors classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "\n",
    "k_neighbors = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "print(\" K | TRAINING ERROR\")\n",
    "for k in k_neighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_trn, y_trn)\n",
    "    y_pred = clf.predict(X_trn)\n",
    "    correct_pred = sum(y_pred == y_trn)\n",
    "    acc = correct_pred / len(y_trn)\n",
    "    e = 1 - acc\n",
    "    print(f\" {k} | {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of O(N * (N * D + NlogN)). For each of x1,..., xN values in X_tst, we get the distance for each D for each x in X_trn (N * (N * D)). Per x in X_tst, we must also sort the distances to get K nearest neighbor and select majority class, giving rise to (N * ((N * D) + NlogN + K + K)), but since K is usally much smaller than N, we can abstract it to the order mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        return\n",
    "\n",
    "    def fit(self, X_trn, y_trn):\n",
    "        self.model.fit(X_trn, y_trn)\n",
    "        # self.model is stored\n",
    "        return\n",
    "\n",
    "    def predict(self, X_val, y_val=None):\n",
    "        # self.model is used\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def cross_validation(classifier, X_trn, y_trn, n_folds=5):\n",
    "    # do stuff here\n",
    "\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    outputs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_trn):\n",
    "        \n",
    "        x_train, x_test = X_trn[train_index], X_trn[test_index]\n",
    "        y_train, y_test = y_trn[train_index], y_trn[test_index]\n",
    "        \n",
    "        \n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred = classifier.predict(x_test)\n",
    "        correct_preds = np.sum(y_pred == y_test)\n",
    "        e = 1 - (correct_preds / len(y_test))\n",
    "        \n",
    "        outputs.append((classifier, e))\n",
    "    \n",
    "    # Return the paired (model and error) for all folds\n",
    "    return outputs # [(model1, error1), (model2, error2), ..., (modelK, errorK)]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAX_DEPTH  FOLD_1_ERROR  FOLD_2_ERROR  FOLD_3_ERROR  FOLD_4_ERROR  FOLD_5_ERROR  AVG_ERROR\n",
      "         1      0.654167      0.661667      0.635833      0.637500      0.630000   0.643833\n",
      "         3      0.527500      0.569167      0.565000      0.547500      0.540000   0.549833\n",
      "         6      0.486667      0.493333      0.497500      0.482500      0.480833   0.488167\n",
      "         9      0.460000      0.499167      0.470833      0.464167      0.463333   0.471500\n",
      "        12      0.480833      0.489167      0.464167      0.463333      0.479167   0.475333\n",
      "        14      0.470000      0.510000      0.478333      0.457500      0.472500   0.477667\n"
     ]
    }
   ],
   "source": [
    "# Usage for the cross_validation function:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3 * 29 * 29))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Perform cross-validation for different max_depth values\n",
    "for max_depth in [1, 3, 6, 9, 12, 14]:\n",
    "    classifier = Classifier(DecisionTreeClassifier(max_depth=max_depth))\n",
    "    N = 5\n",
    "    outputs = cross_validation(classifier, X_trn, y_trn, n_folds=N)\n",
    "    \n",
    "    # Collect the errors for each fold\n",
    "    fold_errors = [out_[1] for out_ in outputs]\n",
    "    avg_error = np.mean(fold_errors)\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append([max_depth] + fold_errors + [avg_error])\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "columns = [\"MAX_DEPTH\", \"FOLD_1_ERROR\", \"FOLD_2_ERROR\", \"FOLD_3_ERROR\", \"FOLD_4_ERROR\", \"FOLD_5_ERROR\", \"AVG_ERROR\"]\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df.to_string(index=False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LAMBDA  FOLD_1_ERROR  FOLD_2_ERROR  FOLD_3_ERROR  FOLD_4_ERROR  FOLD_5_ERROR  AVG_ERROR\n",
      "    0.1      0.458333      0.484167      0.482500      0.472500      0.480833   0.475667\n",
      "    1.0      0.422500      0.427500      0.435000      0.428333      0.412500   0.425167\n",
      "   10.0      0.377500      0.407500      0.392500      0.397500      0.353333   0.385667\n",
      "  100.0      0.342500      0.379167      0.348333      0.361667      0.327500   0.351833\n",
      " 1000.0      0.350833      0.374167      0.352500      0.357500      0.340000   0.355000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "\n",
    "X_trn = X_trn.reshape((6000, 3 * 29 * 29))\n",
    "\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform cross-validation for different lambda values\n",
    "for lambda_val in [0.1, 1, 10, 100, 1000]:\n",
    "    classifier = Classifier(LogisticRegression(penalty='l2', C=1/lambda_val, max_iter=10000, tol=.001))\n",
    "    N = 5\n",
    "    outputs = cross_validation(classifier, X_trn, y_trn, n_folds=N)\n",
    "    \n",
    "    # Collect the errors for each fold\n",
    "    fold_errors = [out_[1] for out_ in outputs]\n",
    "    avg_error = np.mean(fold_errors)\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append([lambda_val] + fold_errors + [avg_error])\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "columns = [\"LAMBDA\", \"FOLD_1_ERROR\", \"FOLD_2_ERROR\", \"FOLD_3_ERROR\", \"FOLD_4_ERROR\", \"FOLD_5_ERROR\", \"AVG_ERROR\"]\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier():\n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        return\n",
    "\n",
    "    def fit(self, X_trn, y_trn):\n",
    "        # Just store X_trn, y_trn\n",
    "        self.model = (X_trn, y_trn)\n",
    "\n",
    "    def predict(self, X_val, y_val=None):\n",
    "        assert hasattr(self, \"model\"), \"No fitted model!\"\n",
    "        # do stuff here (use self.model for prediction)\n",
    "        \n",
    "        def KNN_predict_(X_trn, y_trn, x, K): \n",
    "            # Dictionary to store n: distance pairs\n",
    "            distances = {}\n",
    "            for n in range(len(X_trn)):\n",
    "                # Calculate distance between x and x[n]\n",
    "                distance = np.sqrt(np.sum((x - X_trn[n])**2))\n",
    "                distances[n] = distance\n",
    "            \n",
    "            # Sort distances in ascending order\n",
    "            sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n",
    "\n",
    "            # Get the K nearest neighbors\n",
    "            y_neighbor = [] \n",
    "            for n in range(K):\n",
    "                n_nearest = sorted_distances[n][0]\n",
    "                y_neighbor.append(y_trn[n_nearest])\n",
    "            \n",
    "            # get majority class\n",
    "            c_pred = np.argmax(np.bincount(y_neighbor))\n",
    "            return c_pred\n",
    "        \n",
    "        X_trn, y_trn = self.model\n",
    "        y_pred = []\n",
    "        for x in X_val:\n",
    "            y_pred.append(KNN_predict_(X_trn, y_trn, x, self.n_neighbors))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " K  FOLD_1_ERROR  FOLD_2_ERROR  FOLD_3_ERROR  FOLD_4_ERROR  FOLD_5_ERROR  AVG_ERROR\n",
      " 1      0.470000      0.458333      0.457500      0.440833      0.457500   0.456833\n",
      " 3      0.482500      0.458333      0.465833      0.454167      0.486667   0.469500\n",
      " 5      0.455000      0.441667      0.451667      0.432500      0.465000   0.449167\n",
      " 7      0.438333      0.429167      0.446667      0.426667      0.448333   0.437833\n",
      " 9      0.433333      0.425833      0.433333      0.433333      0.445000   0.434167\n",
      "11      0.430833      0.435000      0.440833      0.427500      0.430833   0.433000\n"
     ]
    }
   ],
   "source": [
    "k_neighbors = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform cross-validation for different k values\n",
    "for k in k_neighbors:\n",
    "    knn = KNNClassifier(k)\n",
    "    knn.fit(X_trn, y_trn)\n",
    "    models = cross_validation(knn, X_trn, y_trn, n_folds=5)\n",
    "    \n",
    "    # Collect the errors for each fold\n",
    "    fold_errors = [model[1] for model in models]\n",
    "    avg_error = np.mean(fold_errors)\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append([k] + fold_errors + [avg_error])\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "columns = [\"K\", \"FOLD_1_ERROR\", \"FOLD_2_ERROR\", \"FOLD_3_ERROR\", \"FOLD_4_ERROR\", \"FOLD_5_ERROR\", \"AVG_ERROR\"]\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "print(df.to_string(index=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model | hyper-parameters | cross-validation avg. error | public leaderboard accuracy\n",
    "\\\\\\\n",
    "Classification Tree | max_depth = 9 | .474 | .57866 \\\\\\\n",
    "Classification Tree | max_depth = 7 | .4743 | x\\\\\\\n",
    "\\\\\\\n",
    "Logistic Regression | lambda = 10 | .388 | .62133\\\\\\\n",
    "Logistic Regression | lambda = 100 | .352 | x\\\\\\\n",
    "\\\\\\\n",
    "KNN Classification | K = 7 | 0.438 | .55733\\\\\\\n",
    "KNN Classification | K = 9 | 0.434 | x\\\\\\\n",
    "KNN Classification | K = 11 | 0.434 | x\\\\\\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "THE FIRST MODEL WAS USED FOR INITIAL SUBMISSION, EXPERIMENTING WITH THE REST AS WELL AS I ACTUALLY THINK BEST PERFORMANCES WOULD BE: Tree with max depth of 9, logistic regression with lambda = 100, KNN with K = 9 or 11.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def write_csv(y_pred, filename):\n",
    "    \"\"\"Write a 1d numpy array to a Kaggle-compatible .csv file\"\"\"\n",
    "    with open(filename, 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Id', 'Category'])\n",
    "        for idx, y in enumerate(y_pred):\n",
    "            csv_writer.writerow([idx, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best tree model on test data and save predictions\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "X_tst = X_tst.reshape((len(X_tst), 3 * 29 * 29))\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 9)\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "write_csv(y_pred, 'tree_predictions_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best logistic regression model on test data and save predictions\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "X_tst = X_tst.reshape((len(X_tst), 3 * 29 * 29))\n",
    "\n",
    "lambda_val = 100\n",
    "clf = LogisticRegression(penalty='l2', C=1/lambda_val, max_iter=10000, tol=.001)\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "write_csv(y_pred, 'logistic_predictions_100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best k nearest neighbor on test data and save predictions\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_trn = data['X_trn']\n",
    "y_trn = data['y_trn']\n",
    "X_tst = data['X_tst']\n",
    "\n",
    "X_trn = X_trn.reshape((len(X_trn), 3 * 29 * 29))\n",
    "X_tst = X_tst.reshape((len(X_tst), 3 * 29 * 29))\n",
    "\n",
    "k = 9\n",
    "clf = KNNClassifier(k)\n",
    "clf.fit(X_trn, y_trn)\n",
    "y_pred = clf.predict(X_tst)\n",
    "write_csv(y_pred, 'knn_predictions_9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Scoring Predictions:\n",
    "\n",
    "\n",
    "1) Tree Classification model: I believe the error range would be similar to the error present in the public set and slightly lower than cross validations errors, as depth of 9 should be balanced to avoid overfitting while trying to minimize error on unseen data. The public set seems to be performing better than cross validation predicted, indicating slightly better performance overall which I'd expect to continue in private set.\n",
    "\n",
    "2) Logistic Regression model: The cross validation classification error is very similar to that of public test set, indicating a decent prediction job by cross validation. I'd expect a similar error range in the private test set, with room for slight variations.\n",
    "\n",
    "3) KNN Classification model: I believe the error would slightly increase on the private set as the performance in public set seems to perform worse than cross validation predicted, indicating that the model seems to struggle more than expected and I would expect the trend to continue in private set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
